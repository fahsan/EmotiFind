{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "# fiveguys_2a.ipynb\n",
    "# Team C: Five Guys: Burgers and Fries\n",
    "# Aditya Jain, Alex Sandoval, Darrell Harvey, and Fasih Ahsan\n",
    "# COM SCI X 450.1 Section 362062\n",
    "# UCLA Extension, Spring 2018\n",
    "# IPython Notebook for Class Project 2b Submission\n",
    "# Script creates an EmotiFind function predictEmotion()\n",
    "# Function input is a UTF-8 .txt file for any article, tweet, etc. Must be in the same folder as this .ipynb. \n",
    "# Function output is standard stream print of predicted emotion values for 8 emotions\n",
    "# Sadness, Anger, Joy, Trust, Fear, Surprise, Disgust, Anticipation\n",
    "# Must also keep in same folder as .ipynb: EmoLex, TrainingSetEmotions, TrainingSetArticles .txt files.\n",
    "# Unzip these files from the submission. \n",
    "# Requires installation of nltk corpus, csv, NumPy, Matplotlib, SciKit-Learn\n",
    "# Version 0.0.2  - 05/14/2018\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "# Import required libraries and packages. \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import default dictionary for dictionary lists, initialize list and dicts. \n",
    "# From collections import defaultdict\n",
    "emotion_dict = defaultdict(list)\n",
    "emolexdatalist= []\n",
    "train_sadness = []\n",
    "train_anger = []\n",
    "train_joy = []\n",
    "train_trust = []\n",
    "train_fear = []\n",
    "train_surprise = []\n",
    "train_disgust = []\n",
    "train_anticipation = []\n",
    "\n",
    "# Open and read the EmoLex into a list of lists. \n",
    "with open(\"NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\", \"r\") as emolexdata:\n",
    "    #Skip the header\n",
    "    next(emolexdata)\n",
    "    #Save data list as a list of lists for each row. \n",
    "    for row in emolexdata:\n",
    "        row = row.strip().split(\"\\t\")\n",
    "        emolexdatalist.append(row)\n",
    "    emolexdata.close()\n",
    "\n",
    "# For each list in the list of lists \n",
    "for line in emolexdatalist:\n",
    "    # If the word is included in the emotion\n",
    "    if line[2] == \"1\":\n",
    "        # And if the emotion is not a sentiment\n",
    "        if (line[1] != 'negative') and (line[1] != 'positive'):\n",
    "            # Add the word to the dict of lists\n",
    "            emotion_dict[line[1]].append(line[0])\n",
    "        else:\n",
    "            # Otherwise if it is 0, and if is a sentiment, skip. \n",
    "            continue\n",
    "            \n",
    "#Open and import the training set emotions.txt\n",
    "with open(\"TrainingSetEmotions.txt\", \"r\") as trainingsetemotions:\n",
    "    # Skip header\n",
    "    next(trainingsetemotions)\n",
    "    # Import file as tab delimited text. \n",
    "    reader = csv.reader(trainingsetemotions,delimiter='\\t')\n",
    "    for sadness,anger,joy,trust,fear,surprise,disgust,anticipation in reader:\n",
    "        #Add each article's predicted emotion value to each list. \n",
    "        # Ex. train_sadness = [Sadness_Article1, Sadness_Article2, ..., Sadness_Article25]\n",
    "        train_sadness.append(sadness)\n",
    "        train_anger.append(anger)\n",
    "        train_joy.append(joy)\n",
    "        train_trust.append(trust)\n",
    "        train_fear.append(fear)\n",
    "        train_surprise.append(surprise)\n",
    "        train_disgust.append(disgust)\n",
    "        train_anticipation.append(anticipation)\n",
    "\n",
    "# Converting the list to a numpy array.         \n",
    "train_sadness = np.asarray(train_sadness)\n",
    "train_anger = np.asarray(train_anger)\n",
    "train_joy = np.asarray(train_joy)\n",
    "train_trust = np.asarray(train_trust)\n",
    "train_fear = np.asarray(train_fear)\n",
    "train_surprise = np.asarray(train_surprise)\n",
    "train_disgust = np.asarray(train_disgust)\n",
    "train_anticipation = np.asarray(train_anticipation)\n",
    "\n",
    "# @function clean_article()\n",
    "# @input articleName is the .txt file of the article to clean. \n",
    "def clean_article(articleName):\n",
    "    token_frequency_dic = {}\n",
    "    with open(articleName,'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "        # split into words\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "\n",
    "        # remove punctuation from each word\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "        # Add code to filter and extract proper nouns and named identities. \n",
    "        \n",
    "        # filter out stop words and sort\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        words.sort()\n",
    "        req = nltk.FreqDist(words)\n",
    "        for k,v in req.items():\n",
    "            token_frequency_dic[str(k)] = v\n",
    "        \n",
    "        \n",
    "        # Return dictionary of frequencies per word\n",
    "        # Ex. dict = {smith : 4, journey : 5, ....}\n",
    "        return token_frequency_dic\n",
    "\n",
    "    f.close()\n",
    "\n",
    "# @function token_checker_1\n",
    "# @input emotion_dict is the EmoLex ({digust: [gross,nasty,slimy], joy : [fantastic, tremendous, yuge]})\n",
    "# @input token_frequency is the token_frequency_dict from cleanArticle()\n",
    "# @output list of frequencies present and absent. \n",
    "# @output Ex. [{disgust: {word1 : 1, word2 : 2,...}}, {tremendous:1, yuge : 2}]\n",
    "#part 1 of checking the words in emption_dict\n",
    "def token_checker_1(emotion_dict, token_frequency):\n",
    "    # Initialize two dictionaries\n",
    "    token_frequency_present = {}\n",
    "    token_frequency_absent = {}\n",
    "    #creating a blank dictionary for each emotion based on emotion_dict\n",
    "    for emotion in emotion_dict:\n",
    "        token_frequency_present[emotion] = {}\n",
    "        # dict{disgust:{}, anger:{}, joy:{},...}\n",
    "        \n",
    "    for word in token_frequency: #taking the word \n",
    "        present = False  #internal check to see if the word is present in any of the emotion dicts\n",
    "        for emotion in emotion_dict: # going into sad in the emotion dict\n",
    "            if word in emotion_dict[emotion]: # going into the list of words in sad words\n",
    "                token_frequency_present[emotion][word]= token_frequency[word]\n",
    "                # dict{disgust:{word:1}, anger:{}, joy:{},...}\n",
    "                present = True #will change if \n",
    "        # If present is still false after running everything:\n",
    "        if present == False:\n",
    "            token_frequency_absent[word]= token_frequency[word] #Same as token_frequency\n",
    "            #dict{tremendous : 1, yuge :2}\n",
    "    return [token_frequency_present,token_frequency_absent]\n",
    "        #[{disgust: {word1 : 1, word2 : 2,...}}, {tremendous:1, yuge : 2}]\n",
    "\n",
    "# @function token_checker_2\n",
    "# @input emotion_dict is the EmoLex ({digust: [gross,nasty,slimy], joy : [fantastic, tremendous, yuge]})\n",
    "# @input token_frequency_present is the frequency of present words from token_checker_1.\n",
    "# @input token_frequency_absent is the frequency of words absent from the initial pass in token_checker_1. \n",
    "# @output list of frequencies present and absent after lemmatization. \n",
    "# @output Ex. [{disgust: {word1 : 1, word2 : 2,...}}, {tremendous:1, yuge : 2}]\n",
    "# part 2 of checking the words in the emotion_dict\n",
    "def token_checker_2(token_frequency_present,token_frequency_absent,emotion_dict):\n",
    "    #Initialize lemmatizer. \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    token_frequency_absent_l = {}\n",
    "    for word in token_frequency_absent: #taking the word sorrow\n",
    "        word_lemma = lemmatizer.lemmatize(word)\n",
    "        # Check each new lemmatized word to the initial present list\n",
    "        for word_p in token_frequency_present:\n",
    "            # If the lemmatized word is literally an emotion (i.e. disgust, fear, joy, etc.)\n",
    "            if word_lemma is word_p:\n",
    "                # Append the emotion to the end of the list for said emotion\n",
    "                # Ex. dict{disgust:{word1:1, word2:2, disgust:1}}\n",
    "                token_frequency_present[word_p] = token_frequency_present[word_p] + token_frequency_absent[word]\n",
    "            else:\n",
    "                token_frequency_absent_l[word_lemma] = token_frequency_absent[word]\n",
    "\n",
    "    [token_frequency_present_2,token_frequency_absent_2] = token_checker_1(emotion_dict,token_frequency_absent_l)\n",
    "\n",
    "    for emotion in token_frequency_present_2:\n",
    "        if emotion == {}:\n",
    "            continue\n",
    "        for word in token_frequency_present_2[emotion]:\n",
    "            token_frequency_present[emotion][word] = token_frequency_present_2[emotion][word]\n",
    "    return [token_frequency_present,token_frequency_absent_2]\n",
    "\n",
    "\n",
    "#part 3 for checking the words in the synonyms:\n",
    "def token_checker_3(token_frequency_present,token_frequency_absent_2,emotion_dict):    \n",
    "    for word in token_frequency_absent_2:\n",
    "        #create a list with all synonyms\n",
    "        syns = wordnet.synsets(word)\n",
    "        syns_words = []\n",
    "        for n in range(len(syns)):\n",
    "            syns_words.append(syns[n].lemmas()[0].name())\n",
    "        #print(syns_words)\n",
    "        # go in each word in syns_words to make comparison\n",
    "        present = False        \n",
    "        token_frequency_absent_3 = {}\n",
    "        for word_s in syns_words:\n",
    "            if present == True:\n",
    "                break\n",
    "                #check if it is token_frequency_present, \n",
    "                #if yes update the frequency and exit all the for loops except the first one\n",
    "            if word_s is token_frequency_present:\n",
    "                token_frequency_present[word_s] = token_frequency_present[word_s] + token_frequency_absent_2[word]\n",
    "                present = True\n",
    "                #print(present)\n",
    "            else:\n",
    "                #if is is absent in token_frequency_present, \n",
    "                #check emotion dictionary and if it is present exit all except first for\n",
    "                for emotion in emotion_dict: # going into sad in the emotion dict\n",
    "                    if word_s in emotion_dict[emotion]: # going into the list of words in sad words\n",
    "                        token_frequency_present[emotion][word]= token_frequency_absent_2[word]\n",
    "                        present = True\n",
    "                        #print(\"present for\", word_s)\n",
    "        #if it is absent in emotion dictionary, add it to token_frequency_present_3\n",
    "        if present == False:\n",
    "            token_frequency_absent_3[word]= token_frequency_absent_2[word]\n",
    "    return [token_frequency_present, token_frequency_absent_3]\n",
    "\n",
    "def create_param(token_frequency_present,token_frequency):    \n",
    "    # First parameter is total count of words in emotion. (absolute emotion)\n",
    "    parameter1 = {}\n",
    "    for emotion in token_frequency_present:\n",
    "        parameter1[emotion] = 0\n",
    "    for emotion in token_frequency_present:\n",
    "        parameter1[emotion] = sum(token_frequency_present[emotion].values())\n",
    "\n",
    "    # Second parameter is total count of words in emotion/total count of total words (frequency of emotion)\n",
    "    parameter2 = {}\n",
    "    total_words = sum(token_frequency.values())\n",
    "    for emotion in parameter1:\n",
    "        parameter2[emotion] = parameter1[emotion]/total_words    \n",
    "    return [parameter1, parameter2]\n",
    "    # Ex. [45, 0.4]\n",
    "    print(parameter1)\n",
    "    # Could have a multicollinearity issue. \n",
    " \n",
    "    #May merge this with article_parameters() if time. \n",
    "def token_checker(emotion_dict,token_frequency):\n",
    "    [token_frequency_present,token_frequency_absent_1] = token_checker_1(emotion_dict,token_frequency)\n",
    "    #print(\"part 1 token_frequency_present is\", token_frequency_present, \"\\n\")\n",
    "    #print(\"part 1 token_frequency_absent is\", token_frequency_absent_1,\"\\n\")\n",
    "    #print(\"part 1 params are\", create_param(token_frequency_present,token_frequency))\n",
    "    [token_frequency_present, token_frequency_absent_2] = token_checker_2(token_frequency_present,token_frequency_absent_1,emotion_dict)\n",
    "    #print(\"part 2 token_frequency_present is\", token_frequency_present,\"\\n\")\n",
    "    #print(\"part 2 token_frequency_absent is\", token_frequency_absent_2, \"\\n\")\n",
    "    #print(\"part 2 params are\", create_param(token_frequency_present,token_frequency))\n",
    "    [token_frequency_present, token_frequency_absent_3] = token_checker_3(token_frequency_present,token_frequency_absent_2,emotion_dict)\n",
    "    #print(\"part 3 token_frequency_present is\", token_frequency_present, \"\\n\")\n",
    "    #print(\"part 3 token_frequency_absent is\", token_frequency_absent_3, \"\\n\")\n",
    "    #print(\"part 3 params are\", create_param(token_frequency_present,token_frequency))\n",
    "    return create_param(token_frequency_present,token_frequency)\n",
    "    # Ex. [45, 0.4]\n",
    "    #print(parameter1)\n",
    "    #print(parameter2)\n",
    "\n",
    "def article_parameters(articleName):\n",
    "    token_frequency = clean_article(articleName)\n",
    "    return token_checker(emotion_dict,token_frequency)\n",
    "\n",
    "# Function predictEmotion()\n",
    "# Create a model by performing article_parameters() 25 times (once per article)\n",
    "# Training Input per emotion: ex. Sadness = [[Article1P1,Article1P2], [Article2P1,Article2P2],...,[Article25P1,Article25P2]]\n",
    "# Training Output per emotion: ex. Sadness = [Article1Emotion, Article2Emotion, Article3Emotion,...Article25Emotion]\n",
    "# Based on the training output values, uses Support Vector Regression to create a best fit predictor\n",
    "# Test the model to predict an emotion for a article given [test_articleP1,test_articleP2] for each emotion. \n",
    "def predictEmotion(articlename):\n",
    "    # Initialize the test lists for the input test article. \n",
    "    # [test_articleP1,test_articleP2]\n",
    "    trust_testlist = []\n",
    "    fear_testlist = []\n",
    "    sadness_testlist = []\n",
    "    anger_testlist = []\n",
    "    surprise_testlist = []\n",
    "    disgust_testlist = []\n",
    "    joy_testlist = []\n",
    "    anticipation_testlist = []\n",
    "    \n",
    "    # Initialize the training list for the 25 training articles. \n",
    "    # Ex. Sadness = [[Article1P1,Article1P2], [Article2P1,Article2P2],...,[Article25P1,Article25P2]]\n",
    "    trust_trainlist = []\n",
    "    fear_trainlist = []\n",
    "    sadness_trainlist = []\n",
    "    anger_trainlist = []\n",
    "    surprise_trainlist = []\n",
    "    disgust_trainlist = []\n",
    "    joy_trainlist = []\n",
    "    anticipation_trainlist = []\n",
    "    article_reader_list = []\n",
    "    \n",
    "    # Create list of articles to implement. \n",
    "    for i in range(25):\n",
    "       article_reader_list.append(\"train_article\" + str(i + 1) + \".txt\")\n",
    "\n",
    "    for article in article_reader_list:\n",
    "        #train_outputs is [test_articleP1,test_articleP2]\n",
    "        train_outputs = article_parameters(article)\n",
    "        #Converts P1{sad:34,joy:42} to listP1[34,42]\n",
    "        listofP1 = list(train_outputs[0].values())\n",
    "        listofP2 = list(train_outputs[1].values())\n",
    "        # [[Article1P1Sad,Article1P2Sad],[Article1P1Joy,Article1P2Joy], ..., [Article1P1Antic, Article1P2Antic]]\n",
    "        param_merge = [list(each_emot) for each_emot in zip(listofP1, listofP2)]\n",
    "\n",
    "        trust_trainlist.append(param_merge[0])\n",
    "        #Ex. Trust = [[Article1P1,Article1P2], [Article2P1,Article2P2],...,[Article25P1,Article25P2]]\n",
    "        fear_trainlist.append(param_merge[1])\n",
    "        sadness_trainlist.append(param_merge[2])\n",
    "        anger_trainlist.append(param_merge[3])\n",
    "        surprise_trainlist.append(param_merge[4])\n",
    "        disgust_trainlist.append(param_merge[5])\n",
    "        joy_trainlist.append(param_merge[6])\n",
    "        anticipation_trainlist.append(param_merge[7])\n",
    "        #Ex. Sadness = [[Article1P1,Article1P2], [Article2P1,Article2P2],...,[Article25P1,Article25P2]]\n",
    "        \n",
    "    # Convert to NumPy array.\n",
    "    trust_trainlist = np.asarray(trust_trainlist)\n",
    "    fear_trainlist = np.asarray(fear_trainlist)\n",
    "    sadness_trainlist = np.asarray(sadness_trainlist)\n",
    "    anger_trainlist = np.asarray(anger_trainlist)\n",
    "    surprise_trainlist = np.asarray(surprise_trainlist)\n",
    "    disgust_trainlist = np.asarray(disgust_trainlist)\n",
    "    joy_trainlist = np.asarray(joy_trainlist)\n",
    "    anticipation_trainlist = np.asarray(anticipation_trainlist)\n",
    "    \n",
    "    # Now generate the parameters for the test article. \n",
    "    test_article = article_parameters(articlename)\n",
    "    testlistofP1 = list(test_article[0].values())\n",
    "    testlistofP2 = list(test_article[1].values())\n",
    "    test_param_merge = [list(each_emot) for each_emot in zip(testlistofP1, testlistofP2)]\n",
    "\n",
    "    #trust = [test_articleP1, test_articleP2]\n",
    "    trust_testlist.append(test_param_merge[0])\n",
    "    fear_testlist.append(test_param_merge[1])\n",
    "    sadness_testlist.append(test_param_merge[2])\n",
    "    anger_testlist.append(test_param_merge[3])\n",
    "    surprise_testlist.append(test_param_merge[4])\n",
    "    disgust_testlist.append(test_param_merge[5])\n",
    "    joy_testlist.append(test_param_merge[6])\n",
    "    anticipation_testlist.append(test_param_merge[7])\n",
    "    \n",
    "    #Convert to Numpy \n",
    "    trust_testlist = np.asarray(trust_testlist)\n",
    "    fear_testlist = np.asarray(fear_testlist)\n",
    "    sadness_testlist = np.asarray(sadness_testlist)\n",
    "    anger_testlist = np.asarray(anger_testlist)\n",
    "    surprise_testlist = np.asarray(surprise_testlist)\n",
    "    disgust_testlist = np.asarray(disgust_testlist)\n",
    "    joy_testlist = np.asarray(joy_testlist)\n",
    "    anticipation_testlist = np.asarray(anticipation_testlist)\n",
    "    \n",
    "    # Create support vector regression and performed fit based on the training lists. \n",
    "    clf = SVR()\n",
    "    sadnessfit = clf.fit(sadness_trainlist, train_sadness)\n",
    "    trustfit = clf.fit(trust_trainlist, train_trust)\n",
    "    fearfit = clf.fit(fear_trainlist, train_fear)\n",
    "    angerfit = clf.fit(anger_trainlist, train_anger)\n",
    "    surprisefit = clf.fit(surprise_trainlist, train_surprise)\n",
    "    disgustfit = clf.fit(disgust_trainlist, train_disgust)\n",
    "    joyfit = clf.fit(joy_trainlist, train_joy)\n",
    "    anticipationfit = clf.fit(anticipation_trainlist, train_anticipation)\n",
    "    \n",
    "    # Create prediction of new value from a test_article using the support vector regression model calculated above. \n",
    "    sadval = sadnessfit.predict(sadness_testlist)\n",
    "    trustval = trustfit.predict(trust_testlist)\n",
    "    fearval = fearfit.predict(fear_testlist)\n",
    "    angerval = angerfit.predict(anger_testlist)\n",
    "    surpriseval = surprisefit.predict(surprise_testlist)\n",
    "    disgustval = disgustfit.predict(disgust_testlist)\n",
    "    joyval = joyfit.predict(joy_testlist)\n",
    "    anticipationval = anticipationfit.predict(anticipation_testlist)\n",
    "    \n",
    "    # Group all values into a list. \n",
    "    valuelist = [sadval[0], angerval[0], joyval[0], trustval[0], fearval[0], surpriseval[0], disgustval[0],\n",
    "                anticipationval[0]]\n",
    "    # Iterate through each value in the list. \n",
    "    # If the value is less than 0, force to 0\n",
    "    # If the value is greater than 1, force to 1. \n",
    "    # Else keep the value and round to two significant digits. \n",
    "    valuelist[:] = [0.00 if value < 0 else 1 if value > 1.00 else value for value in valuelist]\n",
    "    \n",
    "    # Print standard output. \n",
    "    print(\"Thank you for using EmotiFind by the Five Guys! For the article you have selected, the predicted emotions are:\\n\",\n",
    "         \"Sadness:\", valuelist[0], \"\\n\", \n",
    "         \"Anger:\", valuelist[1], \"\\n\", \n",
    "          \"Joy:\", valuelist[2], \"\\n\",\n",
    "          \"Trust:\", valuelist[3], \"\\n\",\n",
    "         \"Fear:\", valuelist[4], \"\\n\",\n",
    "         \"Surprise:\", valuelist[5], \"\\n\",\n",
    "         \"Disgust:\", valuelist[6], \"\\n\",\n",
    "         \"Anticipation:\", valuelist[7], \"\\n\",\n",
    "         \"Thank you! Please try us again soon!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for using EmotiFind by the Five Guys! For the article you have selected, the predicted emotions are:\n",
      " Sadness: 0.0 \n",
      " Anger: 0.250053834601244 \n",
      " Joy: 0.09880697571931346 \n",
      " Trust: 0.23585118113484183 \n",
      " Fear: 0.09880697571931346 \n",
      " Surprise: 0.7489239132192381 \n",
      " Disgust: 0.09880697571931346 \n",
      " Anticipation: 0.10022281411770509 \n",
      " Thank you! Please try us again soon!\n",
      "Thank you for using EmotiFind by the Five Guys! For the article you have selected, the predicted emotions are:\n",
      " Sadness: 0.39950707772081806 \n",
      " Anger: 0.7845356375399681 \n",
      " Joy: 0.2357051357532984 \n",
      " Trust: 0.24727227921377185 \n",
      " Fear: 0.0996744453537744 \n",
      " Surprise: 0.37323015883738736 \n",
      " Disgust: 0.24727173108234368 \n",
      " Anticipation: 0.10009121356669573 \n",
      " Thank you! Please try us again soon!\n",
      "Thank you for using EmotiFind by the Five Guys! For the article you have selected, the predicted emotions are:\n",
      " Sadness: 0.0 \n",
      " Anger: 0.10000166384595949 \n",
      " Joy: 0.24563659580119843 \n",
      " Trust: 0.4000435410883715 \n",
      " Fear: 0.25272418920865497 \n",
      " Surprise: 0.157950246061568 \n",
      " Disgust: 0.24727227697273843 \n",
      " Anticipation: 0.10000166384595949 \n",
      " Thank you! Please try us again soon!\n",
      "Thank you for using EmotiFind by the Five Guys! For the article you have selected, the predicted emotions are:\n",
      " Sadness: 0.3997024744618408 \n",
      " Anger: 0.3730809122270305 \n",
      " Joy: 0.10007989894735608 \n",
      " Trust: 0.24722306241143802 \n",
      " Fear: 0.771192688417598 \n",
      " Surprise: 0.771192688417598 \n",
      " Disgust: 0.7854174696030649 \n",
      " Anticipation: 0.10008832879415794 \n",
      " Thank you! Please try us again soon!\n",
      "Thank you for using EmotiFind by the Five Guys! For the article you have selected, the predicted emotions are:\n",
      " Sadness: 0.1703224320470939 \n",
      " Anger: 0.1703224320470939 \n",
      " Joy: 0.18830198649824037 \n",
      " Trust: 0.2273662089794146 \n",
      " Fear: 0.10001739888580988 \n",
      " Surprise: 0.22551530505602937 \n",
      " Disgust: 0.7832732728040744 \n",
      " Anticipation: 0.2998656198325868 \n",
      " Thank you! Please try us again soon!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-77c2f675944a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_reader_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpredictEmotion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-19a7ddfe53f3>\u001b[0m in \u001b[0;36mpredictEmotion\u001b[0;34m(articlename)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marticle_reader_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;31m#train_outputs is [test_articleP1,test_articleP2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mtrain_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;31m#Converts P1{sad:34,joy:42} to listP1[34,42]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mlistofP1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-19a7ddfe53f3>\u001b[0m in \u001b[0;36marticle_parameters\u001b[0;34m(articleName)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0marticle_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticleName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mtoken_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticleName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtoken_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;31m# Function predictEmotion()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-19a7ddfe53f3>\u001b[0m in \u001b[0;36mtoken_checker\u001b[0;34m(emotion_dict, token_frequency)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;31m#print(\"part 2 token_frequency_absent is\", token_frequency_absent_2, \"\\n\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;31m#print(\"part 2 params are\", create_param(token_frequency_present,token_frequency))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mtoken_frequency_present\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_frequency_absent_3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_checker_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_frequency_present\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken_frequency_absent_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memotion_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m     \u001b[0;31m#print(\"part 3 token_frequency_present is\", token_frequency_present, \"\\n\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;31m#print(\"part 3 token_frequency_absent is\", token_frequency_absent_3, \"\\n\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-19a7ddfe53f3>\u001b[0m in \u001b[0;36mtoken_checker_3\u001b[0;34m(token_frequency_present, token_frequency_absent_2, emotion_dict)\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;31m#check emotion dictionary and if it is present exit all except first for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0memotion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memotion_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# going into sad in the emotion dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mword_s\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memotion_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# going into the list of words in sad words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m                         \u001b[0mtoken_frequency_present\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtoken_frequency_absent_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                         \u001b[0mpresent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_reader_list = []\n",
    "\n",
    "for i in range(25):\n",
    "    test_reader_list.append(\"train_article\" + str(i + 1) + \".txt\")\n",
    "\n",
    "for article in test_reader_list:\n",
    "    predictEmotion(article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
