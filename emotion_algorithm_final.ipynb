{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "# fiveguys_2a.ipynb\n",
    "# Team C: Five Guys: Burgers and Fries\n",
    "# Aditya Jain, Alex Sandoval, Darrell Harvey, and Fasih Ahsan\n",
    "# COM SCI X 450.1 Section 362062\n",
    "# UCLA Extension, Spring 2018\n",
    "# IPython Notebook for Class Project 2b Submission\n",
    "# Script creates an EmotiFind function predictEmotion()\n",
    "# Function input is a UTF-8 .txt file for any article, tweet, etc. Must be in the same folder as this .ipynb. \n",
    "# Function output is standard stream print of predicted emotion values for 8 emotions\n",
    "# Sadness, Anger, Joy, Trust, Fear, Surprise, Disgust, Anticipation\n",
    "# Must also keep in same folder as .ipynb: EmoLex, TrainingSetEmotions, TrainingSetArticles .txt files.\n",
    "# Unzip these files from the submission. \n",
    "# Requires installation of nltk corpus, csv, NumPy, Matplotlib, SciKit-Learn\n",
    "# Version 0.0.1  - 05/13/2018\n",
    "####################################################################################################\n",
    "\n",
    "# Import required libraries and packages. \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import default dictionary for dictionary lists, initialize list and dicts. \n",
    "#from collections import defaultdict\n",
    "emotion_dict = defaultdict(list)\n",
    "emolexdatalist= []\n",
    "\n",
    "# Open and read the EmoLex into a list of lists. \n",
    "with open(\"NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\", \"r\") as emolexdata:\n",
    "    next(emolexdata)\n",
    "    for row in emolexdata:\n",
    "        row = row.strip().split(\"\\t\")\n",
    "        emolexdatalist.append(row)\n",
    "    emolexdata.close()\n",
    "\n",
    "# For each list in the list of lists \n",
    "for line in emolexdatalist:\n",
    "    # If the word is included in the emotion\n",
    "    if line[2] == \"1\":\n",
    "        # And if the emotion is not a sentiment\n",
    "        if (line[1] != 'negative') and (line[1] != 'positive'):\n",
    "            # Add the word to the list of lists\n",
    "            emotion_dict[line[1]].append(line[0])\n",
    "        else:\n",
    "            # Otherwise if it is 0, and if is a sentiment, skip. \n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove named entities\n",
    "def ne_removal(text):\n",
    "    tokens = regexp_tokenize(text, \"[\\w']+\")\n",
    "    chunked = nltk.ne_chunk(nltk.pos_tag(tokens))\n",
    "    tokens = [leaf[0] for leaf in chunked if type(leaf) != nltk.Tree]\n",
    "    return tokens\n",
    "\n",
    "# # @function clean_article\n",
    "# @input articleName is the .txt file of the article to clean. \n",
    "def clean_article(articleName):\n",
    "    token_frequency_dic = {}\n",
    "    with open(articleName,'r') as f:\n",
    "        text = f.read()\n",
    "        tokens = ne_removal(text)\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "\n",
    "        # remove punctuation from each word\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        \n",
    "        # filter out stop words and sort\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        words.sort()\n",
    "        req = nltk.FreqDist(words)\n",
    "        for k,v in req.items():\n",
    "            token_frequency_dic[str(k)] = v\n",
    "        return token_frequency_dic\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#this section of code will generate an array for all assigned values to each article \n",
    "#defines an empty dict with null lists for emotion_trainlist and emotion_testlist\n",
    "\n",
    "train_values = {}\n",
    "emotion_fit = {}\n",
    "for name in emotion_dict:\n",
    "    z = \"train_\" + str(name)\n",
    "    train_values[z] = []\n",
    "    u = str(name) + \"_fit\"\n",
    "    emotion_fit[u] = 0\n",
    "    \n",
    "#Open and import the training set emotions.txt\n",
    "with open(\"TrainingSetEmotions.txt\", \"r\") as trainingsetemotions:\n",
    "    next(trainingsetemotions)\n",
    "    reader = csv.reader(trainingsetemotions,delimiter='\\t')\n",
    "    for sadness,anger,joy,trust,fear,surprise,disgust,anticipation in reader:\n",
    "        train_values['train_sadness'].append(sadness)\n",
    "        train_values['train_anger'].append(anger)\n",
    "        train_values['train_joy'].append(joy)\n",
    "        train_values['train_trust'].append(trust)\n",
    "        train_values['train_fear'].append(fear)\n",
    "        train_values['train_surprise'].append(surprise)\n",
    "        train_values['train_disgust'].append(disgust)\n",
    "        train_values['train_anticipation'].append(anticipation)\n",
    "\n",
    "#convert the lists into numpy arrays\n",
    "train_values['train_sadness'] = np.asarray(train_values['train_sadness'],dtype=float)\n",
    "train_values['train_anger'] = np.asarray(train_values['train_anger'],dtype=float)\n",
    "train_values['train_joy'] = np.asarray(train_values['train_joy'],dtype=float)\n",
    "train_values['train_trust'] = np.asarray(train_values['train_trust'],dtype=float)\n",
    "train_values['train_fear'] = np.asarray(train_values['train_fear'],dtype=float)\n",
    "train_values['train_surprise'] = np.asarray(train_values['train_surprise'],dtype=float)\n",
    "train_values['train_disgust'] = np.asarray(train_values['train_disgust'],dtype=float)\n",
    "train_values['train_anticipation'] = np.asarray(train_values['train_anticipation'],dtype=float)\n",
    "\n",
    "article_reader_list = []\n",
    "for i in range(25):\n",
    "    article_reader_list.append(\"train_article\" + str(i + 1) + \".txt\")\n",
    "\n",
    "#print(train_values)\n",
    "#print(emotion_fit)\n",
    "#print(article_reader_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 1 of checking the words in emption_dict\n",
    "def token_checker_1(emotion_dict, token_frequency):\n",
    "    token_frequency_present = {}\n",
    "    token_frequency_absent = {}\n",
    "    #creating a blank dictionary for each emotion based on emotion_dict\n",
    "    for emotion in emotion_dict:\n",
    "        token_frequency_present[emotion] = {}\n",
    "        \n",
    "    for word in token_frequency: #taking the word \n",
    "        present = False  #internal check to see if the word is present in any of the emotion dicts\n",
    "        for emotion in emotion_dict: # going into sad in the emotion dict\n",
    "            if word in emotion_dict[emotion]: # going into the list of words in sad words\n",
    "                token_frequency_present[emotion][word]= token_frequency[word]\n",
    "                present = True #will change if \n",
    "        if present == False:\n",
    "            token_frequency_absent[word]= token_frequency[word]\n",
    "    return [token_frequency_present,token_frequency_absent]\n",
    "\n",
    "\n",
    "\n",
    "#part 2 of checking the words in the emption_dict\n",
    "def token_checker_2(token_frequency_present,token_frequency_absent,emotion_dict):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    token_frequency_absent_l = {}\n",
    "    for word in token_frequency_absent: #taking the word sorrow\n",
    "        word_lemma = lemmatizer.lemmatize(word)\n",
    "        for word_p in token_frequency_present:\n",
    "            if word_lemma is word_p:\n",
    "                token_frequency_present[word_p] = token_frequency_present[word_p] + token_frequency_absent[word]\n",
    "            else:\n",
    "                token_frequency_absent_l[word_lemma] = token_frequency_absent[word]\n",
    "\n",
    "    [token_frequency_present_2,token_frequency_absent_2] = token_checker_1(emotion_dict,token_frequency_absent_l)\n",
    "\n",
    "    for emotion in token_frequency_present_2:\n",
    "        if emotion == {}:\n",
    "            continue\n",
    "        for word in token_frequency_present_2[emotion]:\n",
    "            token_frequency_present[emotion][word] = token_frequency_present_2[emotion][word]\n",
    "    return [token_frequency_present,token_frequency_absent_2]\n",
    "\n",
    "\n",
    "#part 3 for checking the words in the synonyms:\n",
    "def token_checker_3(token_frequency_present,token_frequency_absent_2,emotion_dict):    \n",
    "    for word in token_frequency_absent_2:\n",
    "        #create a list with all synonyms\n",
    "        syns = wordnet.synsets(word)\n",
    "        syns_words = []\n",
    "        for n in range(len(syns)):\n",
    "            syns_words.append(syns[n].lemmas()[0].name())\n",
    "        #print(syns_words)\n",
    "        # go in each word in syns_words to make comparison\n",
    "        present = False        \n",
    "        token_frequency_absent_3 = {}\n",
    "        for word_s in syns_words:\n",
    "            if present == True:\n",
    "                break\n",
    "                #check if it is token_frequency_present, \n",
    "                #if yes update the frequency and exit all the for loops except the first one\n",
    "            if word_s is token_frequency_present:\n",
    "                token_frequency_present[word_s] = token_frequency_present[word_s] + token_frequency_absent_2[word]\n",
    "                present = True\n",
    "                #print(present)\n",
    "            else:\n",
    "                #if is is absent in token_frequency_present, \n",
    "                #check emotion dictionary and if it is present exit all except first for\n",
    "                for emotion in emotion_dict: # going into sad in the emotion dict\n",
    "                    if word_s in emotion_dict[emotion]: # going into the list of words in sad words\n",
    "                        token_frequency_present[emotion][word]= token_frequency_absent_2[word]\n",
    "                        present = True\n",
    "                        #print(\"present for\", word_s)\n",
    "        #if it is absent in emotion dictionary, add it to token_frequency_present_3\n",
    "        if present == False:\n",
    "            token_frequency_absent_3[word]= token_frequency_absent_2[word]\n",
    "    return [token_frequency_present, token_frequency_absent_3]\n",
    "\n",
    "\n",
    "#will return parameter1 as {'sadness':22,'anger': 10..'anticipation': 4}\n",
    "#will return parameter2 as {'sadness':0.22,'anger':0.1..'anticipation': 0.04}\n",
    "#will return final output as param_merge = [(sadness_p1,p2), (trust_p1,p2), ..(anticipation_p1, p2)]\n",
    "def create_param(token_frequency_present,token_frequency):    \n",
    "    parameter1 = {}\n",
    "    for emotion in token_frequency_present:\n",
    "        parameter1[emotion] = 0\n",
    "    for emotion in token_frequency_present:\n",
    "        parameter1[emotion] = sum(token_frequency_present[emotion].values())\n",
    "\n",
    "    parameter2 = {}\n",
    "    total_words = sum(token_frequency.values())\n",
    "    for emotion in parameter1:\n",
    "        parameter2[emotion] = parameter1[emotion]/total_words   \n",
    "    \n",
    "    param_merge = []\n",
    "    list_p1 = list(parameter1.values())\n",
    "    list_p2 = list(parameter2.values())\n",
    "    param_merge = [list(each_emot) for each_emot in zip(list_p1, list_p2)]\n",
    "    #param_merge = [(sadness_p1,p2), (trust_p1,p2), ..(anticipation_p1, p2)]\n",
    "    return param_merge    \n",
    "\n",
    "# combining all of the above formulae into one single formula    \n",
    "def article_parameters(articleName):\n",
    "    token_frequency = clean_article(articleName)\n",
    "    [token_frequency_present,token_frequency_absent_1] = token_checker_1(emotion_dict,token_frequency)\n",
    "    #print(\"part 1 token_frequency_present is\", token_frequency_present, \"\\n\")\n",
    "    #print(\"part 1 token_frequency_absent is\", token_frequency_absent_1,\"\\n\")\n",
    "    #print(\"part 1 params are\", create_param(token_frequency_present,token_frequency))\n",
    "    [token_frequency_present, token_frequency_absent_2] = token_checker_2(token_frequency_present,token_frequency_absent_1,emotion_dict)\n",
    "    #print(\"part 2 token_frequency_present is\", token_frequency_present,\"\\n\")\n",
    "    #print(\"part 2 token_frequency_absent is\", token_frequency_absent_2, \"\\n\")\n",
    "    #print(\"part 2 params are\", create_param(token_frequency_present,token_frequency))\n",
    "    \n",
    "    #[token_frequency_present, token_frequency_absent_3] = token_checker_3(token_frequency_present,token_frequency_absent_2,emotion_dict)\n",
    "    #print(\"part 3 token_frequency_present is\", token_frequency_present, \"\\n\")\n",
    "    #print(\"part 3 token_frequency_absent is\", token_frequency_absent_3, \"\\n\")\n",
    "    #print(\"part 3 params are\", create_param(token_frequency_present,token_frequency))    \n",
    "    return create_param(token_frequency_present,token_frequency)\n",
    "    #print(parameter1)\n",
    "    #print(parameter2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#will take train_values and list of article as input\n",
    "#will iterate over articles to generate paramenters and store them in train_param_list\n",
    "#will return models based on train_param_list and train_values\n",
    "\n",
    "def train_model(article_reader_list, train_values):    \n",
    "    #define train_param_list and emotion_fit to store values later\n",
    "    train_param_list = {}\n",
    "    emotion_fit = {}\n",
    "    for name in emotion_dict:\n",
    "        x = str(name) + \"_trainlist\"\n",
    "        train_param_list[x] = []\n",
    "        u = str(name) + \"_fit\"\n",
    "        emotion_fit[u] = 0\n",
    "    \n",
    "    #going through all the articles to generate train_param_list with values for all articles\n",
    "    for article in article_reader_list:\n",
    "        #param_merge = [(sadness_p1,p2), (trust_p1,p2), ..(anticipation_p1, p2)]        \n",
    "        param_merge = article_parameters(article)\n",
    "        n = 0\n",
    "        for item in train_param_list:            \n",
    "            train_param_list[item].append(param_merge[n])\n",
    "            n += 1       \n",
    "    #print(train_param_list)\n",
    "    \n",
    "    #generate arrays for regression model\n",
    "    for item in train_param_list:\n",
    "        train_param_list[item] = np.asarray(train_param_list[item],dtype=float)\n",
    "    #print(train_param_list)    \n",
    "    \n",
    "    # generate the regression model for each emotion \n",
    "    clf = SVR()\n",
    "    for (fit,param,values) in zip(emotion_fit, train_param_list, train_values):\n",
    "        emotion_fit[fit] = clf.fit(train_param_list[param], train_values[values])    \n",
    "    return emotion_fit\n",
    "\n",
    "emotion_fit = train_model(article_reader_list, train_values)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in input as an article and generates predicted emotion\n",
    "\n",
    "def predictEmotion(article): \n",
    "    #create empty dicts\n",
    "    test_list = {}\n",
    "    emotion_output_val = {}\n",
    "    for name in emotion_dict:\n",
    "        x = str(name) + \"_testlist\"\n",
    "        test_list[x] = []\n",
    "        y = str(name) + \"_output_value\"\n",
    "        emotion_output_val[y] = 0\n",
    "    \n",
    "    #get parameters for the new articles\n",
    "    test_list = article_parameters(article)\n",
    "    for n in range(8):\n",
    "        test_list[n] = np.asarray(test_list[n],dtype =float)\n",
    "    \n",
    "    #use regression model to generate values\n",
    "    for (val,fit,n) in zip(emotion_output_val, emotion_fit, range(8)):\n",
    "        emotion_output_val[val] = emotion_fit[fit].predict(test_list[n].reshape(1, -1))\n",
    "        if emotion_output_val[val] < 0:\n",
    "            emotion_output_val[val] = [0]\n",
    "        elif emotion_output_val[val] > 1:\n",
    "            emotion_output_val[val] = [1]\n",
    "    \n",
    "    #return emotion_output_val\n",
    "    print (\"Thank you for using EmotiFind by the Five Guys! For the article you have selected, the predicted emotions are\")\n",
    "    for val in emotion_output_val:\n",
    "        print(val,\": \", round(emotion_output_val[val][0],2))\n",
    "    \n",
    "    \n",
    "    #generate chart \n",
    "    emotion = list(emotion_dict.keys())\n",
    "    values = []\n",
    "    for item in emotion_output_val:\n",
    "        values.append(emotion_output_val[item][0])\n",
    "        \n",
    "    xs = [i+0.1 for i, _ in enumerate(emotion)]\n",
    "    plt.bar(xs,values)\n",
    "    plt.xticks([i+0.1 for i in range(8)],emotion) \n",
    "    plt.ylabel(\"Emotion Value\")\n",
    "    plt.ylim(0,1)\n",
    "    plt.xlabel(\"Emotion\")\n",
    "    plt.title(\"Article Emotion Graph\")\n",
    "    plt.plot()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for using EmotiFind by the Five Guys! For the article you have selected, the predicted emotions are\n",
      "trust_output_value :  0.25\n",
      "fear_output_value :  0\n",
      "sadness_output_value :  0.1\n",
      "anger_output_value :  0\n",
      "surprise_output_value :  0.1\n",
      "disgust_output_value :  0\n",
      "joy_output_value :  0.1\n",
      "anticipation_output_value :  0.1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictEmotion(\"train_article21.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
