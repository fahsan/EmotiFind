{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "# fiveguys_2a.ipynb\n",
    "# Team C: Five Guys: Burgers and Fries\n",
    "# Aditya Jain, Alex Sandoval, Darrell Harvey, and Fasih Ahsan\n",
    "# COM SCI X 450.1 Section 362062\n",
    "# UCLA Extension, Spring 2018\n",
    "# IPython Notebook for Class Project 2b Submission\n",
    "# Script creates an EmotiFind function predictEmotion()\n",
    "# Function input is a UTF-8 .txt file for any article, tweet, etc. Must be in the same folder as this .ipynb. \n",
    "# Function output is standard stream print of predicted emotion values for 8 emotions\n",
    "# Sadness, Anger, Joy, Trust, Fear, Surprise, Disgust, Anticipation\n",
    "# Must also keep in same folder as .ipynb: EmoLex, TrainingSetEmotions, TrainingSetArticles .txt files.\n",
    "# Unzip these files from the submission. \n",
    "# Requires installation of nltk corpus, csv, NumPy, Matplotlib, SciKit-Learn\n",
    "# Version 0.0.1  - 05/13/2018\n",
    "####################################################################################################\n",
    "\n",
    "# Import required libraries and packages. \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import default dictionary for dictionary lists, initialize list and dicts. \n",
    "#from collections import defaultdict\n",
    "emotion_dict = defaultdict(list)\n",
    "emolexdatalist= []\n",
    "\n",
    "# Open and read the EmoLex into a list of lists. \n",
    "with open(\"NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\", \"r\") as emolexdata:\n",
    "    next(emolexdata)\n",
    "    for row in emolexdata:\n",
    "        row = row.strip().split(\"\\t\")\n",
    "        emolexdatalist.append(row)\n",
    "    emolexdata.close()\n",
    "\n",
    "# For each list in the list of lists \n",
    "for line in emolexdatalist:\n",
    "    # If the word is included in the emotion\n",
    "    if line[2] == \"1\":\n",
    "        # And if the emotion is not a sentiment\n",
    "        if (line[1] != 'negative') and (line[1] != 'positive'):\n",
    "            # Add the word to the list of lists\n",
    "            emotion_dict[line[1]].append(line[0])\n",
    "        else:\n",
    "            # Otherwise if it is 0, and if is a sentiment, skip. \n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove named entities\n",
    "def ne_removal(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    chunked = nltk.ne_chunk(nltk.pos_tag(tokens))\n",
    "    tokens = [leaf[0] for leaf in chunked if type(leaf) != nltk.Tree]\n",
    "    return tokens\n",
    "\n",
    "# # @function clean_article\n",
    "# @input articleName is the .txt file of the article to clean. \n",
    "def clean_article(articleName):\n",
    "    token_frequency_dic = {}\n",
    "    with open(articleName,'r') as f:\n",
    "        text = f.read()\n",
    "        tokens = ne_removal(text)\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "\n",
    "        # remove punctuation from each word\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        \n",
    "        # filter out stop words and sort\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        words.sort()\n",
    "        req = nltk.FreqDist(words)\n",
    "        for k,v in req.items():\n",
    "            token_frequency_dic[str(k)] = v\n",
    "        return token_frequency_dic\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# defines an empty dict with null lists for emotion_trainlist and emotion_testlist\n",
    "train_values = {}\n",
    "emotion_fit = {}\n",
    "for name in emotion_dict:\n",
    "    z = \"train_\" + str(name)\n",
    "    train_values[z] = []\n",
    "    u = str(name) + \"_fit\"\n",
    "    emotion_fit[u] = 0\n",
    "    \n",
    "#Open and import the training set emotions.txt\n",
    "with open(\"TrainingSetEmotions.txt\", \"r\") as trainingsetemotions:\n",
    "    next(trainingsetemotions)\n",
    "    reader = csv.reader(trainingsetemotions,delimiter='\\t')\n",
    "    for sadness,anger,joy,trust,fear,surprise,disgust,anticipation in reader:\n",
    "        train_values['train_sadness'].append(sadness)\n",
    "        train_values['train_anger'].append(anger)\n",
    "        train_values['train_joy'].append(joy)\n",
    "        train_values['train_trust'].append(trust)\n",
    "        train_values['train_fear'].append(fear)\n",
    "        train_values['train_surprise'].append(surprise)\n",
    "        train_values['train_disgust'].append(disgust)\n",
    "        train_values['train_anticipation'].append(anticipation)\n",
    "\n",
    "#convert the lists into numpy arrays\n",
    "train_values['train_sadness'] = np.asarray(train_values['train_sadness'],dtype=float)\n",
    "train_values['train_anger'] = np.asarray(train_values['train_anger'],dtype=float)\n",
    "train_values['train_joy'] = np.asarray(train_values['train_joy'],dtype=float)\n",
    "train_values['train_trust'] = np.asarray(train_values['train_trust'],dtype=float)\n",
    "train_values['train_fear'] = np.asarray(train_values['train_fear'],dtype=float)\n",
    "train_values['train_surprise'] = np.asarray(train_values['train_surprise'],dtype=float)\n",
    "train_values['train_disgust'] = np.asarray(train_values['train_disgust'],dtype=float)\n",
    "train_values['train_anticipation'] = np.asarray(train_values['train_anticipation'],dtype=float)\n",
    "\n",
    "article_reader_list = []\n",
    "for i in range(25):\n",
    "    article_reader_list.append(\"train_article\" + str(i + 1) + \".txt\")\n",
    "\n",
    "#print(train_values)\n",
    "#print(emotion_fit)\n",
    "#print(article_reader_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @function\n",
    "    #part 1 of checking the words in emption_dict\n",
    "def token_checker_1(emotion_dict, token_frequency):\n",
    "    token_frequency_present = {}\n",
    "    token_frequency_absent = {}\n",
    "    #creating a blank dictionary for each emotion based on emotion_dict\n",
    "    for emotion in emotion_dict:\n",
    "        token_frequency_present[emotion] = {}\n",
    "        \n",
    "    for word in token_frequency: #taking the word \n",
    "        present = False  #internal check to see if the word is present in any of the emotion dicts\n",
    "        for emotion in emotion_dict: # going into sad in the emotion dict\n",
    "            if word in emotion_dict[emotion]: # going into the list of words in sad words\n",
    "                token_frequency_present[emotion][word]= token_frequency[word]\n",
    "                present = True #will change if \n",
    "        if present == False:\n",
    "            token_frequency_absent[word]= token_frequency[word]\n",
    "    return [token_frequency_present,token_frequency_absent]\n",
    "\n",
    "\n",
    "\n",
    "#part 2 of checking the words in the emption_dict\n",
    "def token_checker_2(token_frequency_present,token_frequency_absent,emotion_dict):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    token_frequency_absent_l = {}\n",
    "    for word in token_frequency_absent: #taking the word sorrow\n",
    "        word_lemma = lemmatizer.lemmatize(word)\n",
    "        for word_p in token_frequency_present:\n",
    "            if word_lemma is word_p:\n",
    "                token_frequency_present[word_p] = token_frequency_present[word_p] + token_frequency_absent[word]\n",
    "            else:\n",
    "                token_frequency_absent_l[word_lemma] = token_frequency_absent[word]\n",
    "\n",
    "    [token_frequency_present_2,token_frequency_absent_2] = token_checker_1(emotion_dict,token_frequency_absent_l)\n",
    "\n",
    "    for emotion in token_frequency_present_2:\n",
    "        if emotion == {}:\n",
    "            continue\n",
    "        for word in token_frequency_present_2[emotion]:\n",
    "            token_frequency_present[emotion][word] = token_frequency_present_2[emotion][word]\n",
    "    return [token_frequency_present,token_frequency_absent_2]\n",
    "\n",
    "\n",
    "#part 3 for checking the words in the synonyms:\n",
    "def token_checker_3(token_frequency_present,token_frequency_absent_2,emotion_dict):    \n",
    "    for word in token_frequency_absent_2:\n",
    "        #create a list with all synonyms\n",
    "        syns = wordnet.synsets(word)\n",
    "        syns_words = []\n",
    "        for n in range(len(syns)):\n",
    "            syns_words.append(syns[n].lemmas()[0].name())\n",
    "        #print(syns_words)\n",
    "        # go in each word in syns_words to make comparison\n",
    "        present = False        \n",
    "        token_frequency_absent_3 = {}\n",
    "        for word_s in syns_words:\n",
    "            if present == True:\n",
    "                break\n",
    "                #check if it is token_frequency_present, \n",
    "                #if yes update the frequency and exit all the for loops except the first one\n",
    "            if word_s is token_frequency_present:\n",
    "                token_frequency_present[word_s] = token_frequency_present[word_s] + token_frequency_absent_2[word]\n",
    "                present = True\n",
    "                #print(present)\n",
    "            else:\n",
    "                #if is is absent in token_frequency_present, \n",
    "                #check emotion dictionary and if it is present exit all except first for\n",
    "                for emotion in emotion_dict: # going into sad in the emotion dict\n",
    "                    if word_s in emotion_dict[emotion]: # going into the list of words in sad words\n",
    "                        token_frequency_present[emotion][word]= token_frequency_absent_2[word]\n",
    "                        present = True\n",
    "                        #print(\"present for\", word_s)\n",
    "        #if it is absent in emotion dictionary, add it to token_frequency_present_3\n",
    "        if present == False:\n",
    "            token_frequency_absent_3[word]= token_frequency_absent_2[word]\n",
    "    return [token_frequency_present, token_frequency_absent_3]\n",
    "\n",
    "\n",
    "#will return parameter1 as {'sadness':22,'anger': 10..'anticipation': 4}\n",
    "#will return parameter2 as {'sadness':0.22,'anger':0.1..'anticipation': 0.04}\n",
    "#will return final output as param_merge = [(sadness_p1,p2), (trust_p1,p2), ..(anticipation_p1, p2)]\n",
    "def create_param(token_frequency_present,token_frequency):    \n",
    "    parameter1 = {}\n",
    "    for emotion in token_frequency_present:\n",
    "        parameter1[emotion] = 0\n",
    "    for emotion in token_frequency_present:\n",
    "        parameter1[emotion] = sum(token_frequency_present[emotion].values())\n",
    "\n",
    "    parameter2 = {}\n",
    "    total_words = sum(token_frequency.values())\n",
    "    for emotion in parameter1:\n",
    "        parameter2[emotion] = parameter1[emotion]/total_words   \n",
    "    \n",
    "    param_merge = []\n",
    "    list_p1 = list(parameter1.values())\n",
    "    list_p2 = list(parameter2.values())\n",
    "    param_merge = [list(each_emot) for each_emot in zip(list_p1, list_p2)]\n",
    "    #param_merge = [(sadness_p1,p2), (trust_p1,p2), ..(anticipation_p1, p2)]\n",
    "    return param_merge    \n",
    "\n",
    "    \n",
    "def article_parameters(articleName):\n",
    "    token_frequency = clean_article(articleName)\n",
    "    [token_frequency_present,token_frequency_absent_1] = token_checker_1(emotion_dict,token_frequency)\n",
    "    #print(\"part 1 token_frequency_present is\", token_frequency_present, \"\\n\")\n",
    "    #print(\"part 1 token_frequency_absent is\", token_frequency_absent_1,\"\\n\")\n",
    "    #print(\"part 1 params are\", create_param(token_frequency_present,token_frequency))\n",
    "    [token_frequency_present, token_frequency_absent_2] = token_checker_2(token_frequency_present,token_frequency_absent_1,emotion_dict)\n",
    "    #print(\"part 2 token_frequency_present is\", token_frequency_present,\"\\n\")\n",
    "    #print(\"part 2 token_frequency_absent is\", token_frequency_absent_2, \"\\n\")\n",
    "    #print(\"part 2 params are\", create_param(token_frequency_present,token_frequency))\n",
    "    \n",
    "    #[token_frequency_present, token_frequency_absent_3] = token_checker_3(token_frequency_present,token_frequency_absent_2,emotion_dict)\n",
    "    #print(\"part 3 token_frequency_present is\", token_frequency_present, \"\\n\")\n",
    "    #print(\"part 3 token_frequency_absent is\", token_frequency_absent_3, \"\\n\")\n",
    "    #print(\"part 3 params are\", create_param(token_frequency_present,token_frequency))    \n",
    "    return create_param(token_frequency_present,token_frequency)\n",
    "    #print(parameter1)\n",
    "    #print(parameter2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#will take train_values and list of article as input\n",
    "#will iterate over articles to generate paramenters and store them in train_param_list\n",
    "#will return models based on train_param_list and train_values\n",
    "\n",
    "def train_model(article_reader_list, train_values):    \n",
    "    #define train_param_list and emotion_fit to store values later\n",
    "    train_param_list = {}\n",
    "    emotion_fit = {}\n",
    "    for name in emotion_dict:\n",
    "        x = str(name) + \"_trainlist\"\n",
    "        train_param_list[x] = []\n",
    "        u = str(name) + \"_fit\"\n",
    "        emotion_fit[u] = 0\n",
    "    \n",
    "    #going through all the articles to generate train_param_list with values for all articles\n",
    "    for article in article_reader_list:\n",
    "        #param_merge = [(sadness_p1,p2), (trust_p1,p2), ..(anticipation_p1, p2)]        \n",
    "        param_merge = article_parameters(article)\n",
    "        n = 0\n",
    "        for item in train_param_list:            \n",
    "            train_param_list[item].append(param_merge[n])\n",
    "            n += 1       \n",
    "    #print(train_param_list)\n",
    "    \n",
    "    #generate arrays\n",
    "    for item in train_param_list:\n",
    "        train_param_list[item] = np.asarray(train_param_list[item],dtype=float)\n",
    "    #print(train_param_list)    \n",
    "    \n",
    "    # generate the regression model for each emotion \n",
    "    clf = SVR()\n",
    "    for (fit,param,values) in zip(emotion_fit, train_param_list, train_values):\n",
    "        emotion_fit[fit] = clf.fit(train_param_list[param], train_values[values])    \n",
    "    return emotion_fit\n",
    "\n",
    "emotion_fit = train_model(article_reader_list, train_values)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14, 0.03753351206434316], [17, 0.045576407506702415], [14, 0.03753351206434316], [9, 0.024128686327077747], [12, 0.032171581769437], [10, 0.02680965147453083], [16, 0.04289544235924933], [24, 0.064343163538874]]\n",
      "[0.09922396]\n",
      "Thank you for using EmotiFind by the Five Guys! For the article you have selected, the predicted emotions are\n",
      "trust_output_value :  [0.09922396]\n",
      "fear_output_value :  [0.0521031]\n",
      "sadness_output_value :  [0.09922396]\n",
      "anger_output_value :  [0.32977695]\n",
      "surprise_output_value :  [0.34531819]\n",
      "disgust_output_value :  [0.09995097]\n",
      "joy_output_value :  [0.10020712]\n",
      "anticipation_output_value :  [0.10019489]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predictEmotion(article): \n",
    "    test_list = {}\n",
    "    emotion_output_val = {}\n",
    "    for name in emotion_dict:\n",
    "        x = str(name) + \"_testlist\"\n",
    "        test_list[x] = []\n",
    "        y = str(name) + \"_output_value\"\n",
    "        emotion_output_val[y] = 0\n",
    "        \n",
    "    test_list = article_parameters(article)\n",
    "    print(test_list)\n",
    "    for n in range(8):\n",
    "        test_list[n] = np.asarray(test_list[n],dtype =float)\n",
    "    \n",
    "    emotion_output_val['sadness_output_value'] = emotion_fit['sadness_fit'].predict(test_list[0].reshape(1, -1))\n",
    "    print(emotion_output_val['sadness_output_value'])\n",
    "    \n",
    "    for (val,fit,n) in zip(emotion_output_val, emotion_fit, range(8)):\n",
    "        emotion_output_val[val] = emotion_fit[fit].predict(test_list[n].reshape(1, -1))\n",
    "    \n",
    "    print (\"Thank you for using EmotiFind by the Five Guys! For the article you have selected, the predicted emotions are\")\n",
    "    for val in emotion_output_val:\n",
    "        print(val,\": \", emotion_output_val[val])\n",
    "    #return emotion_output_val\n",
    "\n",
    "predictEmotion(\"train_article12.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
